% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/general_find_duplicates.R
\name{find_duplicates}
\alias{find_duplicates}
\title{Find duplicated files and directories within a given path}
\usage{
find_duplicates(
  path = ".",
  size_threshold = 0L,
  extensions = NULL,
  n_cores = 1L,
  print_results = TRUE
)
}
\arguments{
\item{path}{Character. Root directory to scan for duplicates.}

\item{size_threshold}{Numeric. Minimum file size (in MB) to report as
duplicate.}

\item{extensions}{Character vector. Optional file extensions (without the
leading dot) to filter (case-insensitive); e.g., \code{c("csv", "txt")}). If
provided, only files with these extensions are considered. . Directories
are excluded if they don't match. Defaults to \code{NULL} (all files are
considered).}

\item{n_cores}{Integer. Number of parallel workers to use (default: 1).}

\item{print_results}{Logical. Whether to print results to the console
(default: \code{TRUE}).}
}
\value{
A list of tibbles with duplicated files and directories, if found.
The \code{duplicated_files} tibble  (if any) contains the following columns:
\itemize{
\item \code{path}: root path scanned;
\item \code{dup_group}: duplicate group ID;
\item \code{files}: list-column of tibbles with absolute/relative paths and
modification times;
\item \code{file_ext}: file extension(s) of the group;
\item \code{n_files}: number of duplicated files in the group;
\item \code{file_size_mb}: size (MB) of the first file in the group; and
\item \code{content_hash}: MD5 hash of the file content.
The \code{duplicated_dirs} tibble (if any) contains the following columns:
\item \code{dir}: directory path at the duplicated level;
\item \code{dir_abs}: absolute path to the directory;
\item \code{n_files}: number of files in the directory;
\item \code{n_dup_dirs}: number of duplicated directories in the group;
\item \code{dup_group}: unique identifier for each group of duplicated directories.
}
}
\description{
This function scans a directory tree for duplicated files (by content hash)
and optionally duplicated directories (by identical file sets). It supports
filtering by file extension, minimum file size, and parallel processing.
}
\author{
Ahmed El-Gabbas
}
